---
title: "BIOS735 Final Project Report"
author: "Group 5: Anil Anderson, Arthi Hariharan, David Hu, √Ålvaro Quijano, Yumei Yang"
date: "5/1/2024"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
---

# Introduction
The global prevalence of diabetes is rapidly increasing, with projections suggesting that by 2045, 783 million people will be living with diabetes. Type 2 Diabetes, in particular, is a major cause of severe health complications such as blindness, kidney failure, heart attacks, stroke, and lower limb amputation. 

Early diagnosis is crucial for mitigating the effects of diabetes. And a predictive model is an indispensable tool for timely interventions and efficient allocation of. 

Obesity is known to be positively associated with diabetes, while young age is often thought to be a protective factor against it. However, research also shows that prevalence of diabetes in people under 40 years old has been increasing. It is unclear whether there is an association between diabetes and body mass index (BMI), and how this might be impacted by age. 

In this project, we aim to identify the risk factors for predicting Type 2 diabetes and assess the role of BMI and age in Type 2 diabetes using data from a cohort study of Chinese adults.

## Dataset

This dataset comes from a Chinese cohort study conducted in 2016 with a median follow-up time of 3.1 years. Participants were free of diabetes at baseline and a total of 4174 participants have developed diabetes by the end of the study.

## Project Objectives

(1). To identify the risk factors for Type 2 diabetes

(2). To access the role of age and BMI in Type 2 diabetes

(3). To compare predictive models based on penalized logistic regression, Bayesian-based penalized logistic regression models

# Data 

### Overview

Our data is highly unbalanced, with less than 2% of diabetes cases and more than 98% non-diabetes cases:

```{r}
library(predict.bios735)
library(tidyverse)
data <- diabetes
diabetes$Gender <- factor(diabetes$Gender, level = c(1, 2), labels = c("Male", "Female"))
diabetes$diabetes <- factor(diabetes$diabetes, level = c(0, 1), labels = c("Non-diabetes", "Diabetes"))
diabetes$history <- factor(diabetes$history, level = c(0, 1), labels = c("N", "Y"))

# descriptive stats
table1::table1(~ Age + Gender + height + weight + BMI + SBP + DBP + Cholesterol 
               + Triglyceride + HDL + LDL + AST + BUN + CCR + smoke + drink + 
                 history | diabetes, data = diabetes, render.continuous=c(.="Mean (SD)"))
```

LDL, HDL, AST, smoking status, and drinking status have more than 40% missingness, and interestingly, missingness of LDL-HDL, smoking-drinkingalmost always occurred simultaneously. Since the data was pulled from a medical record databse, we assumed that those variables were missing completely at random (MCAR) and imputated the missing values for out further analysis. 

```{r}
# missing patterns
diabetes |>
  select(HDL, LDL, AST, smoke, drink) |>
  naniar::gg_miss_upset()
```

Unsurprisingly, we found the following sets of highly correlated variables:
- weight and height/BMI
- gender and CRR/height/weight
- cholesterol and LDL
- AST and ALT


```{r}
# correlation heatmap
corr_mat <- round(cor(select(data, -c(id, diabetes, FPG, FPG_final)), use = "complete.obs"),2)
melted_corr_mat <- reshape2::melt(corr_mat)

ggplot(melted_corr_mat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  scale_fill_gradient(low = "#FFDD95", high = "#4c9cd3", guide = "colorbar") +
  labs(x = "", y = "")
```

Based on the missing patterns and correlation, we removed HDL and LDL from our analysis because they are correlated with cholesterol and had more than 40% missingness. We also dropped weight variable because we believe that it can be fully represented by height and BMI combined.

### Prepocessing 
We split our data at a 80:20 ratio, adn then impuated the missing variables.
```{r}
library(mice)
library(caret)
data <- diabetes[, -1]
# split the data
set.seed(1349)
train_index <- createDataPartition(data$diabetes, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# impute training set and test set
diabetes_train <- mice(data = train_data, seed = 1349, print = FALSE, m = 1) |> complete()
use_data(diabetes_train, overwrite = TRUE)

diabetes_test <- mice(data = test_data, seed = 1349, print = FALSE, m = 1) |> complete()
use_data(diabetes_test, overwrite = TRUE)

diabetes_complete <- na.omit(data)
use_data(diabetes_complete, overwrite = TRUE)

```

# Methods 



### Model specification



### Likelihood


# Details in methods

We used Ridge-penalized logistic regression, Bayesian methods, and Random Survival Forest to estimate parameters.

### Penalized Ridge logistic regression


### Bayesian methods


### Random Survival Forest


# Software implementation

We write our package `jm5`. Here are the main functions:

```{r eval=FALSE}

```

# Results 


### Summary statistics for `diabetes` dataset

```{r echo=FALSE}

```



### Model fit for `diabetes` dataset - Penalized Ridge regression 

```{r, results = "hide"}

```

It is time-consuming to run MCEM and each iteration takes more than 1 minute. Although the algorithm is hard to converge and we only run 10 iterations, the estimates are still similar with the estimates given by Bayesian method below.

### Model fit for `diabetes` dataset - Bayesian method

```{r warning=FALSE, results="hide"}

```

### Random Forest for `diabetes` dataset

```{r}

```


### Cross-validation: AUC and F1 score for `diabetes` dataset


```{r}

```

# Discussion



## Main conclusion


## Limitations

Below are some advantages and disadvantages of the three methods.

### Penalized Ridge regression 



### Bayesian method


### RSF


## Future direction


# References 

